{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous Multiagent Decision Making\n",
    "This tutorial demonstrates how to configure and train a multiagent environment in RLlib\n",
    "in which homogeneous agents act asyncronously while learning learning a single policy.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Warning:** Part of RLlib's backend mishandles the potential for zero-length episodes,\n",
    "which this method may produce. If this PR is unmerged, it must be manually applied to your \n",
    "installation: https://github.com/ray-project/ray/pull/46721\n",
    "\n",
    "</div>\n",
    "\n",
    "This example was run with the following version of RLlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "version(\"ray\")  # Parent package of RLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Environment\n",
    "A simple multi-satellite environment is defined. This environment is trivial for the \n",
    "multiagent case since there is no communication or interaction between satellites, but it \n",
    "serves to demonstrate asynchronous behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bsk_rl import act, data, obs, sats, scene\n",
    "from bsk_rl.sim import dyn, fsw\n",
    "\n",
    "class ScanningDownlinkDynModel(dyn.ContinuousImagingDynModel, dyn.GroundStationDynModel):\n",
    "    # Define some custom properties to be accessed in the state\n",
    "    @property\n",
    "    def instrument_pointing_error(self) -> float:\n",
    "        r_BN_P_unit = self.r_BN_P/np.linalg.norm(self.r_BN_P) \n",
    "        c_hat_P = self.satellite.fsw.c_hat_P\n",
    "        return np.arccos(np.dot(-r_BN_P_unit, c_hat_P))\n",
    "    \n",
    "    @property\n",
    "    def solar_pointing_error(self) -> float:\n",
    "        a = self.world.gravFactory.spiceObject.planetStateOutMsgs[\n",
    "            self.world.sun_index\n",
    "        ].read().PositionVector\n",
    "        a_hat_N = a / np.linalg.norm(a)\n",
    "        nHat_B = self.satellite.sat_args[\"nHat_B\"]\n",
    "        NB = np.transpose(self.BN)\n",
    "        nHat_N = NB @ nHat_B\n",
    "        return np.arccos(np.dot(nHat_N, a_hat_N))\n",
    "\n",
    "class ScanningSatellite(sats.AccessSatellite):\n",
    "    observation_spec = [\n",
    "        obs.SatProperties(\n",
    "            dict(prop=\"storage_level_fraction\"),\n",
    "            dict(prop=\"battery_charge_fraction\"),\n",
    "            dict(prop=\"wheel_speeds_fraction\"),\n",
    "            dict(prop=\"instrument_pointing_error\", norm=np.pi),\n",
    "            dict(prop=\"solar_pointing_error\", norm=np.pi)\n",
    "        ),\n",
    "        obs.OpportunityProperties(\n",
    "            dict(prop=\"opportunity_open\", norm=5700),\n",
    "            dict(prop=\"opportunity_close\", norm=5700),\n",
    "            type=\"ground_station\",\n",
    "            n_ahead_observe=1,\n",
    "        ),\n",
    "        obs.Eclipse(norm=5700),\n",
    "    ]\n",
    "    action_spec = [\n",
    "        act.Scan(duration=150.0),\n",
    "        act.Charge(duration=120.0),\n",
    "        act.Downlink(duration=80.0),\n",
    "        act.Desat(duration=45.0),\n",
    "    ]\n",
    "    dyn_type = ScanningDownlinkDynModel\n",
    "    fsw_type = fsw.ContinuousImagingFSWModel\n",
    "\n",
    "sats = [ScanningSatellite(\n",
    "    f\"Scanner-{i+1}\",\n",
    "    sat_args=dict(\n",
    "        # Data\n",
    "        dataStorageCapacity=5000 * 8e6,  # bits\n",
    "        storageInit=lambda: np.random.uniform(0.0, 0.8) * 5000 * 8e6,\n",
    "        instrumentBaudRate=0.5 * 8e6,\n",
    "        transmitterBaudRate=-50 * 8e6,\n",
    "        # Power\n",
    "        batteryStorageCapacity=200 * 3600,  # W*s\n",
    "        storedCharge_Init=lambda: np.random.uniform(0.3, 1.0) * 200 * 3600,\n",
    "        basePowerDraw=-10.0,  # W\n",
    "        instrumentPowerDraw=-30.0,  # W\n",
    "        transmitterPowerDraw=-25.0,  # W\n",
    "        thrusterPowerDraw=-80.0,  # W\n",
    "        panelArea=0.25,\n",
    "        # Attitude\n",
    "        imageAttErrorRequirement=0.1,\n",
    "        imageRateErrorRequirement=0.1,\n",
    "        disturbance_vector=lambda: np.random.normal(scale=0.0001, size=3),  # N*m\n",
    "        maxWheelSpeed=6000.0,  # RPM\n",
    "        wheelSpeeds=lambda: np.random.uniform(-3000, 3000, 3),\n",
    "        desatAttitude=\"nadir\",\n",
    "    )\n",
    ") for i in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlated Environment Parameters\n",
    "To construct a constellation with some coordinated, a function is generated to map satellites\n",
    "to orbital elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bsk_rl.utils.orbital import walker_delta_args\n",
    "\n",
    "sat_arg_randomizer = walker_delta_args(n_planes=2, altitude=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sat_arg_randomizer` is included in the environment arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 5 * 5700.0  # About 5 orbits\n",
    "env_args = dict(\n",
    "    satellites=sats,\n",
    "    scenario=scene.UniformNadirScanning(value_per_second=1/duration),\n",
    "    rewarder=data.ScanningTimeReward(),\n",
    "    time_limit=duration,\n",
    "    failure_penalty=-1.0,\n",
    "    terminate_on_time_limit=True,\n",
    "    sat_arg_randomizer=sat_arg_randomizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLlib Training Configuration\n",
    "\n",
    "A standard PPO configuration is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bsk_rl.utils.rllib  # noqa To access \"ConstellationTasking-RLlib\"\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "\n",
    "N_CPUS = 3\n",
    "\n",
    "training_args = dict(\n",
    "    lr=0.00003,\n",
    "    gamma=0.99997,\n",
    "    train_batch_size=200 * N_CPUS,\n",
    "    num_sgd_iter=10,\n",
    "    lambda_=0.95,\n",
    "    use_kl_loss=False,\n",
    "    clip_param=0.1,\n",
    "    grad_clip=0.5,\n",
    "    mini_batch_size_per_learner=100,\n",
    ")\n",
    "\n",
    "config = (\n",
    "    PPOConfig()\n",
    "    .environment(\n",
    "        \"ConstellationTasking-RLlib\",\n",
    "        env_config=env_args,\n",
    "    )\n",
    "    .env_runners(\n",
    "        num_env_runners=N_CPUS - 1,\n",
    "        sample_timeout_s=1000.0,\n",
    "    )\n",
    "    .reporting(\n",
    "        metrics_num_episodes_for_smoothing=1,\n",
    "        metrics_episode_collection_timeout_s=180,\n",
    "    )\n",
    "    .checkpointing(export_native_model_files=True)\n",
    "    .framework(framework=\"torch\")\n",
    "    .api_stack(\n",
    "        enable_rl_module_and_learner=True,\n",
    "        enable_env_runner_and_connector_v2=True,\n",
    "    )\n",
    "    .training(\n",
    "        **training_args,\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up multiple agents using the same policy, the following configurations are set to\n",
    "map all agents to the policy `p0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "    from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "except (ImportError, ModuleNotFoundError):  # Older versions of RLlib\n",
    "    from ray.rllib.core.rl_module.marl_module import (\n",
    "        MultiAgentRLModuleSpec as MultiRLModuleSpec,\n",
    "    )\n",
    "    from ray.rllib.core.rl_module.rl_module import (\n",
    "        SingleAgentRLModuleSpec as RLModuleSpec,\n",
    "    )\n",
    "\n",
    "config.multi_agent(\n",
    "    policies={\"p0\"},\n",
    "    policy_mapping_fn=lambda *args, **kwargs: \"p0\",\n",
    ").rl_module(\n",
    "    model_config_dict={\n",
    "        \"use_lstm\": False,\n",
    "        # Use a simpler FCNet when we also have an LSTM.\n",
    "        \"fcnet_hiddens\": [2048, 2048],\n",
    "        \"vf_share_layers\": False,\n",
    "    },\n",
    "    rl_module_spec=MultiRLModuleSpec(\n",
    "        module_specs={\n",
    "            \"p0\": RLModuleSpec(),\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Multiagent Logging Callbacks\n",
    "A callback function for the entire environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_metrics_callback(env):\n",
    "    reward = env.rewarder.cum_reward\n",
    "    reward = sum(reward.values()) / len(reward)\n",
    "    return dict(reward=reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and per satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sat_metrics_callback(env, satellite):\n",
    "    data = dict(\n",
    "        # Are satellites dying, and how and when?\n",
    "        alive=float(satellite.is_alive()),\n",
    "        rw_status_valid=float(satellite.dynamics.rw_speeds_valid()),\n",
    "        battery_status_valid=float(satellite.dynamics.battery_valid()),\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "are defined. The `sat_metrics_callback` will be reported per-agent and as a mean. If\n",
    "using the predefined `\"ConstellationTasking-RLlib\"`, only the `WrappedEpisodeDataCallbacks`\n",
    "need to be added to the config, as in the single-agent case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bsk_rl.utils.rllib.callbacks import WrappedEpisodeDataCallbacks\n",
    "\n",
    "config.callbacks(WrappedEpisodeDataCallbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Continuation and Concatenation\n",
    "Logic to prevent all agents from retasking whenever any agent finishes an action\n",
    "is introduced, in the form of connector modules. First, the `ContinuePreviousAction` connector\n",
    "overrides any policy-selected action with the `bsk_rl.NO_ACTION` whenever `requires_retasking==False`\n",
    "for an agent, causing the agent to continue its current action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bsk_rl.utils.rllib import discounting\n",
    "\n",
    "config.env_runners(\n",
    "    module_to_env_connector=lambda env: (discounting.ContinuePreviousAction(),)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, two other connectors compress `NO_ACTION` out of episodes of experience, combining\n",
    "steps into those with super-actions. The `d_ts` timestep flag is calculated accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.training(\n",
    "    learner_connector=lambda obs_space, act_space: (\n",
    "        discounting.MakeAddedStepActionValid(expected_train_batch_size=config.train_batch_size),\n",
    "        discounting.CondenseMultiStepActions(),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the `TimeDiscountedGAEPPOTorchLearner` is used, as in :doc:`examples/time_discounted_gae`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.training(learner_class=discounting.TimeDiscountedGAEPPOTorchLearner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when using these connectors, only the `requires_retasking` flag will case agents\n",
    "to select a new action. Step timeouts due to `max_step_duration` will not trigger retasking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Agent\n",
    "At this point, the PPO config can be trained as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    num_cpus=N_CPUS,\n",
    "    object_store_memory=2_000_000_000,  # 2 GB\n",
    ")\n",
    "\n",
    "# Run the training\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    config=config.to_dict(),\n",
    "    stop={\"training_iteration\": 2},  # Adjust the number of iterations as needed\n",
    ")\n",
    "\n",
    "# Shutdown Ray\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_refactor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
